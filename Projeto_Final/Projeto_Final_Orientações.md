# Descrição

•	Todas as equipes deverão entregar as mesmas especificações, de acordo com o seu respectivo tema.

•	Vocês deverão aplicar os conceitos vistos durante o curso para tratar, organizar e modelar os dados de 2 datasets escolhidos por vocês seguindo o tema de sua equipe.

•	Obrigatoriamente deverá conter as tecnologias Google Cloud Platform (Cloud Storage), Python, Pandas, PySpark, SparkSQL, Apache Beam*, Data Studio, Big Query.


# Apresentação

A apresentação do trabalho se dará da seguinte maneira:

Cada grupo deverá ser totalmente responsável pela forma pela qual vai interpretar o dataset, apresentando suposições e conclusões dos dados. Todas essas situações devem ser explicadas.

1.	Deverá iniciar pela apresentação do dataset, informando de qual local foi baixado o dataset e quais as principais informações sobre o mesmo.
2.	Deverá apresentar as funções e ferramentas utilizadas no código.
3.	Explicar o porquê do dataset escolhido.
4.	Todos os componentes deverão se apresentar.
5.	Deverá ser usado termos técnicos, evitando o uso de gírias ou expressões coloquiais e/ou culturais.
6.	Cada grupo terá 60 minutos para se apresentar.
7.	A ordem da apresentação será comunicada pelos professores próximo à data de apresentação.

# Principais Habilidades a serem avaliadas

1.	Oralidade e comunicação em público.
2.	Capacidade de argumentação
3.	Habilidade de codificação em Python
4.	Habilidade de interpretação e análise de dados.
5.	Capacidade de implementação de códigos utilizando as bibliotecas Pandas e PySpark.
6.	Capacidade de implementação de consultas utilizando a linguagem SQL.
7.	Capacidade Analítica e Interpretativa.

# REQUISITOS OBRIGATÓRIOS

●	Obrigatoriamente os datasets devem ter formatos diferentes (CSV / Json / Parquet / Sql / NoSql) e 1 deles obrigatoriamente tem que ser em CSV.

●	Operações com Pandas (limpezas , transformações e normalizações) 

●	Operações usando PySpark com a descrição de cada uma das operações.

●	Operações utilizando o SparkSQL com a descrição de cada umas das operações.

●	Os datasets utilizados podem ser em lingua estrangeira , mas devem ao final terem seus dados/colunas exibidos na lingua PT-BR

●	os datasets devem ser salvos e operados em armazenamento cloud obrigatoriamente dentro da plataforma GCP (não pode ser usado Google drive ou armazenamento alheio ao google)

●	os dados tratados devem ser armazenados também em GCP, mas obrigatoriamente em um datalake(Gstorage ) , DW(BigQuery) ou em ambos.

●	Deve ser feito análises dentro do Big Query utilizando a linguagem padrão SQL com a descrição das consultas feitas.

●	Deve ser criado no datastudio um dash board simples para exibição gráfica dos dados tratados trazendo insights importantes

●	E deve ser demonstrado em um workflow simples (gráfico) as etapas de ETL.




# REQUISITOS DESEJÁVEIS

●	Implementar captura e ingestão de dados por meio de uma PIPELINE com modelo criado em apache beam usando o dataflow para o work

●	Criar plotagens usando pandas para alguns insights durante o processo de Transformação 

●	Por meio de uma PIPELINE fazer o carregamento dos dados normalizados diretamente para um DW ou DataLake ou ambos

●	Montar um relatório completo com os insights que justificam todo o processo de ETL utilizado


